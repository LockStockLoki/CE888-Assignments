%template for producing IEEE-format articles using LaTeX.
%written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
%use at your own risk.  Complaints to /dev/null.
%make two column with no page numbering, default is 10 point
\documentstyle[twocolumn]{article}
\pagestyle{empty}

%set dimensions of columns, gap between columns, and space between paragraphs
\setlength{\textheight}{8.75in}
\setlength{\columnsep}{2.0pc}
\setlength{\textwidth}{6.8in}
\setlength{\footheight}{0.0in}
\setlength{\topmargin}{0.25in}
\setlength{\headheight}{0.0in}
\setlength{\headsep}{0.0in}
\setlength{\oddsidemargin}{-.19in}
\setlength{\parindent}{1pc}

\makeatletter

\def\@normalsize{\@setsize\normalsize{12pt}\xpt\@xpt
\abovedisplayskip 10pt plus2pt minus5pt\belowdisplayskip \abovedisplayskip
\abovedisplayshortskip \z@ plus3pt\belowdisplayshortskip 6pt plus3pt
minus3pt\let\@listi\@listI} 

\def\subsize{\@setsize\subsize{12pt}\xipt\@xipt}

\def\section{\@startsection {section}{1}{\z@}{24pt plus 2pt minus 2pt}
{12pt plus 2pt minus 2pt}{\large\bf}}

\def\subsection{\@startsection {subsection}{2}{\z@}{12pt plus 2pt minus 2pt}
{12pt plus 2pt minus 2pt}{\subsize\bf}}
\makeatother

\begin{document}
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large\bf MCTS}

\author{Robert Marc James-Stroud \\
School of Computer Science and Electronic Engineering \\
University of Essex \\
Colchester, UK \\
Email: rj18801@essex.ac.uk}
 
\maketitle
\thispagestyle{empty}

\subsection*{\centering Abstract}
%IEEE allows italicized abstract
{\em
To be written.  
%end italics mode
}

\section{Introduction}
Monte Carlo Tree Search (MCTS) has received much interest from researchers in multiple areas, including General Game Playing (GGP) \cite{b1},  where it has been used to great success.

MCTS is an algorithm for returning a decision by creating a search tree of the domain, constructed by taking random samples of the search space. MCTS does not require domain knowledge to function although it may be helpful \cite{b2}.

Research into Game AI is important, prior to MCTS databases were used to hold all possible game states. An example of database is presented by \cite{b4}, a game of {\em American Checkers} in which there are 3.9x10$^{13}$ entries, or states. Decision trees used in GGP and General Video Game Playing (GVGP) suffer from dimensionality. 

A simple game, Noughts and Crosses has a branching factor of 4, and a full tree has 10 levels \cite{bartle}, compared with Chess which has a branching factor of 35 and rarely a full search tree \cite{bartle}. Noughts and Crosses has unique states numbering in the thousands, it is clear that crafting rules for each different state is not feasible, especially when the branching factor in other games used in GGP/GVGP can be in the hundreds. 

Firstly this paper will look at what MCTS is and previous work conducted in the field of Game AI, specifically MCTS. Secondly MCTS will be used in a Noughts and Crosses (also known as Os and Xs, OXO and Tic-Tac-Toe).

\subsection{Monte Carlo Tree Search}
MCTS is a tree search algorithm that constructs an asymmetric search tree based on actions, the tree is a biased representation towards more promising areas of the search space \cite{b5}.

Through self-play MCTS estimates the value of a node from that point until it reaches a terminal node \cite{b5}. A node is a state, and the action is the move made that results in that state.  

Each node might be visited multiple times having its value adjusted accordingly. A node that looks promising on the first run through could look less promising as it is visited more times, or become less promising in relation to other branches. 

\begin{enumerate}
  \item {\em Selection} - Using a policy traverse the search tree. Starting at a root node, the current state, select child nodes with promising values until a leaf node is reached.
  \item {\em Expansion} - Once {\em Selection} has selected a leaf node, expand the current node and select one of its children so long as the current node is not terminal.   
  \item {\em Simulation} - Using the node expanded in the {\em Expansion} phase and a policy if defined, playout the game. {\em Playout} in this context means until completion or a result is achieved.
  \item {\em Backpropogation} - After {\em Simulation} is complete the search tree is traversed in reverse order, propagating up the tree the result, updating the value of the nodes, this continues until the current node is the root node. The new values are then used in the {\em Selection} process.
\end{enumerate}

MCTS will continue to cycle through these phases until it is stopped, or an `execution budget is reached' \cite{b6}. Three ways are presented to achieve this, each with their own detraction:
\begin{itemize}
  \item Time - cull MCTS after a certain amount of time has lapsed - if stopped too quickly it may not evaluate nodes accurately.
  \item Depth Culling- Stop MCTS after hit reaches a certain depth limit in the tree - stopping with depth may stop the search from ever reaching a terminal node.
  \item Iteration Limit - Give it a predefined number of cycles, after which it will return a result. One cycle is all four phases of MCTS.
\end{itemize}

Depending on when or how MCTS is stopped will effect how good the estimation is of the action to take. Using depth culling or an iteration limit is useful when comparing algorithms across inconsistent hardware and programming languages.

\subsection{Previous Work}
MCTS has received much interest from researchers, becoming somewhat of an umbrella term covering any implementation of MCTS. Browne {\em et al.} in \cite{b2} summarise the different MCTS implementations up to 2011.

MCTS has been applied in co-operative scenarios \cite{b5}, Real-time games \cite{b2}, Non-deterministic games \cite{b7} and numerous non-game applications \cite{b2}.

Game AI research used to focus on two-player zero-sum games of perfect information with alternating turns\cite{b2}, every two-player zero-sum game has a solution \cite{bartle}. Two main ways of evaluating states in a minimax scenario is to use heuristics, or statistics. The statistics approach can utilise MCTS to run thousands of playouts to find the optimal strategy \cite{bartle}.

\subsubsection{Upper Confidence Bounds}
UCB1 is a bandit algorithm with a logarithmic regret, proposed by Auer {\em et al.} \cite{ucb1}. In a multiarmed bandit problem the `empirically best action' should be taken as often as possible \cite{ucb1}. 

However less explored options should not be ignored in favour of exploitation. Disregarding exploration may leave better actions unexplored and thus increase regret. 

UCB1 policy dictates that arm $j$ maximises the average reward \cite{ucb1}:
\[UCB1 = \overline{X_{j}} + \sqrt{\frac{2 \ln n}{n_{j}}}\]

Where $\overline{X_{j}}$ is the average reward from arm $j$, $n_{j}$ is the number of times arm $j$ was played and $n$ is the total number of plays.

Reward $\overline{X_{j}}$ encourages exploitation of high-reward choices \cite{ucb1} and $\sqrt{\frac{2 \ln n}{2_{j}}}$ encourages exploration of less visited actions \cite{b2}.

\subsubsection{Upper Confidence Bounds for Trees}
Upper Confidence Bounds for Trees (UCT) is a well known and popular algorithm in the MCTS family \cite{b2}. Kocis and Szepesvari \cite{kocsze} proposed applying UCB1 to MCTS.

UCB1 is efficient and simple \cite{b2} and a `promising candidate to address the exploration-exploitation dilemma in MCTS' \cite{b2}.

When an action is selected it can be modelled as a multiarmed bandit problem

The result of combining UCB1 and MCTS is UCT.

\section{Methodology}
Noughts and Crosses is a two-player zero sum game, as mentioned previously it will always have a solution. A search tree for UCT can be constructed using a state-action model, where the state is the game board and the action is the move made. This will result in a new state. 

\section{Experiments}
\section{Results}
\section{Conclusion}
\section{plan}

\begin{thebibliography}{9}

\bibitem{b1}
C. F. Sironi, J. Liu, D. Perez-Liebana, R. D. Gaina, I. Bravi, S. M. Lucas, M. H, M. Winands, ``Self-Adaptive MCTS for General Video Game Playing''. 

\bibitem{b2}
C. Browne, E. Powley, D. Whitehouse, S. Lucas, P. I. Cowling, P. Rohlfshagen, S. Taverner, D. Perez, S. Samothrakis, S. Colton,  
``A Survey of Montro Carlo Tree Search Methods,'' 
{\em IEEE Transactions on Computational Intelligence and AI in Games}, Vol. 4, 2012.

\bibitem{bartle}
R. A. Bartle ``Game Theory,'' {\em CE810 Game Design Lecture 5}, 6 November 2018. 

\bibitem{ucb1}
P. Auer, N. Cesa-Bianchi, P. Fischer, ``Finite-Time Analysis of the Multiarmed Bandit Problem,'' {\em Machine Learning}, Vol. 47, 2002.

\bibitem{kocsze}
L. Kocsis, C. Szepesvari, ``Bandit Based Monte-Carlo Planning'' 

\bibitem{b4}
C. Browne, E. Powley, D. Whitehouse, S. Lucas, P. I. Cowling, P. Rohlfshagen, S. Taverner, D. Perez, S. Samothrakis, S. Colton,  
``EvoMCTS: A Scalable Approach for General Game Learning,'' 
{\em IEEE Transactions on Computational Intelligence and AI in Games}, Vol. 6, 2014.

\bibitem{b5}
P. R. Williams, J. Walton-Rivers, D. Perez-Liebana, S. M. Lucas, ``Monte Carlo Tree Search Applied to Co-operative Problems,'' {\em 2015 7th Computer Science and Electronic Engineering Conference (CEEC)}.

\bibitem{b6}
R. D. Gaina, S. M. Lucas, D. Perez-Liebana, ``Rolling Horizon Evolution Enhancements in General Video Game Playing, '' {\em IEEE Conference on Computational Intelligence and Games}, 2017.

\bibitem{b7}
P. I. Cowling, E. J. Powley, D. Whitehouse, 
``Information Set Monte Carlo Tree Search'' 
{\em IEEE Transactions on Computational Intelligence and AI in Games}, Vol. 4, 2012.


%Unused references
%\bibitem{b7}
%M. C. Fu,
%``Monte Carlo Tree Search: A Tutorial,'' 
%{\em 2018 Winter Simulation Conference}, 2018.
\end{thebibliography}
\end{document}
